{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QIO3UIZe6wsZ"
   },
   "source": [
    "# CIS 419/519 \n",
    "#**Homework 4 : Adaboost and the Challenge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pjPfIJ5G52It"
   },
   "source": [
    "# Adaboost-SAMME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from sklearn import tree\n",
    "\n",
    "class BoostedDT:\n",
    "\n",
    "    def __init__(self, numBoostingIters=100, maxTreeDepth=3):\n",
    "        '''\n",
    "        Constructor\n",
    "\n",
    "        Class Fields \n",
    "        clfs : List object containing individual DecisionTree classifiers, in order of creation during boosting\n",
    "        betas : List of beta values, in order of creation during boosting\n",
    "        '''\n",
    "\n",
    "        self.clfs = None  # keep the class fields, and be sure to keep them updated during boosting\n",
    "        self.betas = None \n",
    "        \n",
    "        #TODO\n",
    "        self.numBoostingIters = numBoostingIters\n",
    "        self.maxTreeDepth = maxTreeDepth\n",
    "        self.weight = []\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, X, y, random_state=None):\n",
    "        '''\n",
    "        Trains the model. \n",
    "        Be sure to initialize all individual Decision trees with the provided random_state value if provided.\n",
    "        \n",
    "        Arguments:\n",
    "            X is an n-by-d Pandas Data Frame\n",
    "            y is an n-by-1 Pandas Data Frame\n",
    "            random_seed is an optional integer value\n",
    "        '''\n",
    "        #TODO\n",
    "        n,d = X.shape\n",
    "        # initialize the weight vectors\n",
    "        w = np.ones(n)/n\n",
    "        self.clfs = []\n",
    "        self.betas = []\n",
    "        K = len(pd.unique(y))\n",
    "\n",
    "        def isWrong(y_pre,y_real):\n",
    "            if y_pre==y_real:\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "\n",
    "        # adaboost-samme\n",
    "        for i in range(self.numBoostingIters):\n",
    "            clfs = tree.DecisionTreeClassifier(criterion='entropy',max_depth=self.maxTreeDepth)\n",
    "            self.clfs.append(clfs)\n",
    "            weak_model = clfs.fit(X,y,sample_weight=w)    # train the weak model with instance weights \n",
    "            yhat = weak_model.predict(X)\n",
    "            train_error = sum((yhat!=y)*w)    # compute the weighted training error of hypothesis\n",
    "            beta = 1/2*(np.log((1-train_error)/train_error)+np.log(K-1))    # choose beta\n",
    "            y_np = y.to_numpy().flatten()\n",
    "            self.betas.append(beta)\n",
    "\n",
    "            # update all instance weights\n",
    "            for j in range(n):\n",
    "                w[j] = w[j]*np.exp(-beta*isWrong(yhat[j],y_np[j]))\n",
    "            w = w/w.sum()    # normalized data\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Used the model to predict values for each instance in X\n",
    "        Arguments:\n",
    "            X is an n-by-d Pandas Data Frame\n",
    "        Returns:\n",
    "            an n-by-1 Pandas Data Frame of the predictions\n",
    "        '''\n",
    "        #TODO\n",
    "        X_np = X.to_numpy()\n",
    "        n,d = X_np.shape\n",
    "        prediction = 0\n",
    "        for i in range(self.numBoostingIters):\n",
    "            yhat_proba = self.clfs[i].predict_proba(X_np)\n",
    "            weight_proba = np.multiply(self.betas[i],yhat_proba)\n",
    "            prediction +=weight_proba\n",
    "        \n",
    "        return np.argmax(prediction,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2DAfYDnGU9l8"
   },
   "source": [
    "# Test BoostedDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def test_boostedDT():\n",
    "\n",
    "    # load the data set\n",
    "    sklearn_dataset = datasets.load_breast_cancer()\n",
    "    # convert to pandas df\n",
    "    df = pd.DataFrame(sklearn_dataset.data,columns=sklearn_dataset.feature_names)\n",
    "    df['CLASS'] = pd.Series(sklearn_dataset.target)\n",
    "    df.head()\n",
    "\n",
    "    # split randomly into training/testing\n",
    "    train, test = train_test_split(df, test_size=0.5, random_state=42)\n",
    "    # Split into X,y matrices\n",
    "    X_train = train.drop(['CLASS'], axis=1)\n",
    "    y_train = train['CLASS']\n",
    "    X_test = test.drop(['CLASS'], axis=1)\n",
    "    y_test = test['CLASS']\n",
    "\n",
    "\n",
    "    # train the decision tree\n",
    "    modelDT = DecisionTreeClassifier()\n",
    "    modelDT.fit(X_train, y_train)\n",
    "\n",
    "    # train the boosted DT\n",
    "    modelBoostedDT = BoostedDT(numBoostingIters=100, maxTreeDepth=2)\n",
    "    modelBoostedDT.fit(X_train, y_train)\n",
    "\n",
    "    # train sklearn's implementation of Adaboost\n",
    "    modelSKBoostedDT = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=100)\n",
    "    modelSKBoostedDT.fit(X_train, y_train)\n",
    "\n",
    "    # output predictions on the test data\n",
    "    ypred_DT = modelDT.predict(X_test)\n",
    "    ypred_BoostedDT = modelBoostedDT.predict(X_test)\n",
    "    ypred_SKBoostedDT = modelSKBoostedDT.predict(X_test)\n",
    "\n",
    "    # compute the training accuracy of the model\n",
    "    accuracy_DT = accuracy_score(y_test, ypred_DT)\n",
    "    accuracy_BoostedDT = accuracy_score(y_test, ypred_BoostedDT)\n",
    "    accuracy_SKBoostedDT = accuracy_score(y_test, ypred_SKBoostedDT)\n",
    "\n",
    "    print(\"Decision Tree Accuracy = \"+str(accuracy_DT))\n",
    "    print(\"My Boosted Decision Tree Accuracy = \"+str(accuracy_BoostedDT))\n",
    "    print(\"Sklearn's Boosted Decision Tree Accuracy = \"+str(accuracy_SKBoostedDT))\n",
    "    print()\n",
    "    print(\"Note that due to randomization, your boostedDT might not always have the \")\n",
    "    print(\"exact same accuracy as Sklearn's boostedDT.  But, on repeated runs, they \")\n",
    "    print(\"should be roughly equivalent and should usually exceed the standard DT.\")\n",
    "\n",
    "test_boostedDT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw4_skeleton.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
